# smartm2m Custom Logging Library

통합로깅 시스템을 위한 터미널에이전트 로깅포맷 라이브러리입니다.

## 특징

- XML 설정 불필요 (jar 추가만으로 동작)
- 기존 로그 시스템 보존 (log4j.xml, logback.xml 그대로 유지)
- 선택적 Fluentd/Kafka 전송
- Java 6 ~ 11+ 지원
- Log4j 1.2 / Logback 모두 지원
- 비동기 Kafka 전송 (성능 영향 없음)
- Thread-safe 보장
- 모든 로그는 JSON 형태로 통일된 형식으로 출력

## 설치

### Maven

**1. Repository 설정**

```xml
<repositories>
    <!-- Nexus Repository (com.smartm2m 그룹용) -->
    <repository>
        <id>nexus-public</id>
        <name>Nexus Public Repository</name>
        <url>https://portit.smartm2m.co.kr/nexus/repository/maven-public/</url>
        <releases>
            <enabled>true</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
```

**2. 의존성 추가**

```xml
<dependency>
    <groupId>com.smartm2m</groupId>
    <artifactId>unified-logging</artifactId>
    <version>[version]</version>
</dependency>
```

**참고:** 이 라이브러리에 Kafka client가 포함되어 있어 별도로 추가할 필요가 없습니다.  
(프로젝트에 이미 다른 버전의 Kafka client가 있는 경우, 버전 충돌이 발생할 수 있습니다.)

## 사용법

### 기본 사용

```java
import com.smartm2m.logging.CustomLogger;

public class MyService {
    private final CustomLogger logger = new CustomLogger(getClass());
    
    public void myMethod() {
        // 기본 메서드 - LOG_MODE에 따라 자동으로 Fluentd/Kafka 전송
        logger.info("로그 메시지");
        logger.warn("경고 메시지");
        logger.error("에러 메시지");
        logger.debug("디버그 메시지");
        
        // 가변인자 지원 (SLF4J 스타일 {} 플레이스홀더)
        logger.error("[BPA] getCopinoInfo ERROR: msgSeq={}", dto.getMsgSeq());
        logger.info("value1={}, value2={}", value1, value2);
        logger.warn("User {} logged in from {}", username, ipAddress);
        
        // 메시지 분리: ":" 기준으로 앞부분은 message, 뒷부분은 logData
        logger.info("CopinoService [NIFI REV] : CopinoService(a=1,b=2)");
        // → message: "CopinoService [NIFI REV]", logData: " CopinoService(a=1,b=2)"
        
        // ":" 없으면 전체 메시지가 logData로 저장
        logger.info("test message");
        // → message: "", logData: "test message"
    }
}
```

**중요:** 
- `LOG_MODE` 환경변수에 따라 자동으로 Fluentd/Kafka 전송됩니다.
- `LOG_MODE=KAFKA`: Kafka 전송
- `LOG_MODE=FLUENTD`: Fluentd 전송 (|fluentd| 프리픽스 추가)
- `LOG_MODE` 미설정: JSON 출력만
- 메시지에 ":"가 있으면 자동으로 분리 (앞부분 → message, 뒷부분 → logData)

### 메서드 체이닝 - Fluentd 전송

```java
// Map 데이터와 함께 Fluentd 전송
Map<String, Object> logData = new HashMap<String, Object>();
logData.put("pinNo", "12345");
logData.put("userName", "홍길동");

logger.message("사용자 로그인")
    .logData(logData)           // Map, String, DTO 모두 가능
    .serviceType("TSS")         // 환경변수 > 메서드 체이닝 > null
    .terminal("HJNC")           // 환경변수 > 메서드 체이닝 > null
    .eventType("GATE_IN")      // "GATEIN"으로 변환 (언더바 제거, 대문자)
    .info()
    .fluentd();                 // |fluentd| 프리픽스 추가하여 stdout 출력

// String 데이터와 함께 Fluentd 전송
logger.message("작업 완료")
    .logData("처리된 레코드 수: 100")
    .serviceType("API")
    .eventType("GATE_OUT")
    .warn()
    .fluentd();

// DTO와 함께 Fluentd 전송
UserDto userDto = new UserDto();
userDto.setPinNo("12345");
logger.message("사용자 정보 조회")
    .logData(userDto)          // DTO는 자동으로 Map으로 변환
    .info()
    .fluentd();
```

### 메서드 체이닝 - Kafka 전송

```java
// Map 데이터와 함께 Kafka 전송
Map<String, Object> logData = new HashMap<String, Object>();
logData.put("docKey", "DOC001");
logData.put("amount", 50000);

logger.message("주문 처리 완료")
    .logData(logData)
    .serviceType("TSS")
    .terminal("HJNC")
    .eventType("ORDER")
    .info()
    .kafka();                  // JSON 출력 + Kafka 전송

// 관제시스템 노출 설정
logger.message("프로세스 시작")
    .logData(logData)
    .serviceType("TSS")
    .info()
    .controlView()             // 관제시스템에 노출 (controlViewYn="Y")
    .kafka();

// 관제시스템 노출 안 함 (기본값)
logger.message("작업 완료")
    .logData(logData)
    .info()
    .kafka();                  // controlViewYn="N"

// DTO와 함께 Kafka 전송
OrderDto orderDto = new OrderDto();
logger.message("주문 생성")
    .logData(orderDto)         // DTO는 자동으로 Map으로 변환
    .error()
    .kafka();

// 가변인자 지원 (메서드 체이닝에서도 사용 가능)
logger.message("주문 처리: orderId={}, amount={}", orderId, amount)
    .logData(logData)
    .serviceType("TSS")
    .info()
    .kafka();

// LOG_MODE 환경변수 사용 (자동 전송)
// export LOG_MODE=KAFKA
logger.message("주문 처리 완료")
    .logData(logData)
    .info();  // kafka()를 호출하지 않아도 자동으로 Kafka 전송됨

// 관제시스템 노출 설정
logger.message("프로세스 시작")
    .logData(logData)
    .serviceType("TSS")
    .info()
    .controlView()             // 관제시스템에 노출 (controlViewYn="Y")
    .kafka();

// 관제시스템 노출 안 함 (기본값)
logger.message("작업 완료")
    .logData(logData)
    .info()
    .kafka();                  // controlViewYn="N"
```

### 로그 레벨

```java
// 기본 메서드 (JSON 형태로 출력)
logger.info("로그메시지");
logger.warn("로그메시지");
logger.error("로그메시지");
logger.debug("로그메시지");

// 메서드 체이닝
logger.message("로그메시지")
    .logData(mapData)
    .info()
    .fluentd();

logger.message("로그메시지")
    .logData(mapData)
    .warn()
    .kafka();

// 또는 level() 메서드 사용
logger.message("로그메시지")
    .logData(mapData)
    .level("ERROR")
    .fluentd();
```

### 서비스 정보 설정

**방법 1: 환경변수 (권장)**

```bash
export SERVICE_TERMINAL=HJNC
export SERVICE_TYPE=TSS
```

```java
// 환경변수에서 자동으로 읽어옴
logger.message("작업 완료")
    .logData(mapData)
    .info()
    .fluentd();
```

**방법 2: 메서드 체이닝으로 설정**

```java
// 메서드 체이닝으로 환경변수 오버라이드
logger.message("작업 완료")
    .logData(mapData)
    .serviceType("API")        // 환경변수 무시하고 이 값 사용
    .terminal("TERM1")         // 환경변수 무시하고 이 값 사용
    .eventType("GATE_IN")     // eventType 설정
    .info()
    .fluentd();
```

**방법 3: 어노테이션 사용**

```java
import com.smartm2m.logging.EventType;
import com.smartm2m.logging.ServiceType;
import com.smartm2m.logging.LogMode;

// eventType, serviceType, logMode 모두 어노테이션으로 설정
@EventType("GATE_IN")
@ServiceType("TSS")
@LogMode("KAFKA")
public void processGateIn() {
    CustomLogger logger = new CustomLogger(getClass());
    
    // eventType, serviceType이 자동으로 설정됨
    // kafka()를 호출하지 않아도 LOG_MODE=KAFKA이면 자동으로 Kafka 전송됨
    logger.message("게이트 입차 처리")
        .logData(mapData)
        .info();  // 자동으로 Kafka 전송됨
}

// 메서드 체이닝이 어노테이션보다 우선
@EventType("GATE_IN")
@ServiceType("TSS")
@LogMode("FLUENTD")
public void processGateIn() {
    CustomLogger logger = new CustomLogger(getClass());
    
    // 메서드 체이닝으로 설정한 값이 우선 (어노테이션 무시)
    logger.message("게이트 입차 처리")
        .logData(mapData)
        .eventType("GATE_OUT")   // 이 값이 사용됨
        .serviceType("API")      // 이 값이 사용됨
        .info()
        .kafka();                 // 이 값이 사용됨 (어노테이션 무시)
}
```

**방법 4: LOG_MODE 환경변수 사용 (자동 전송)**

```bash
export LOG_MODE=KAFKA  # 또는 FLUENTD
```

```java
// LOG_MODE 환경변수가 설정되어 있으면 fluentd()/kafka()를 호출하지 않아도 자동으로 전송됨
logger.message("작업 완료")
    .logData(mapData)
    .info();  // LOG_MODE=KAFKA이면 자동으로 Kafka 전송, LOG_MODE=FLUENTD이면 자동으로 Fluentd 전송
```

**⚠️ 중요:**
- 메서드 체이닝(`logger.message().info()`)을 사용할 때:
  - `LOG_MODE` 환경변수나 `@LogMode` 어노테이션이 없으면 **아무것도 동작하지 않습니다** (JSON 출력도 전송도 안 됨)
  - 전송이 필요하면 반드시 `.fluentd()` 또는 `.kafka()`를 명시적으로 호출하거나, `LOG_MODE` 환경변수/`@LogMode` 어노테이션을 설정하세요
- 기본 메서드(`logger.info()`)는 `LOG_MODE` 환경변수에 따라 자동으로 Fluentd/Kafka 전송됩니다

**우선순위:**
- `serviceType`: 메서드 체이닝 > `@ServiceType` 어노테이션 > 환경변수 > null
- `terminal`: 환경변수 > 메서드 체이닝 > null
- `eventType`: 메서드 체이닝 > `@EventType` 어노테이션 > null
- `logMode` (fluentd/kafka 자동 호출): 메서드 체이닝 (fluentd()/kafka() 호출) > `@LogMode` 어노테이션 > 환경변수 `LOG_MODE` > null

## 환경변수

### 선택 (서비스 정보)

```bash
SERVICE_TERMINAL=HJNC         # 터미널/IDC 식별자
SERVICE_TYPE=TSS             # 서비스 타입
LOG_MODE=KAFKA               # 로그 전송 모드 (FLUENTD 또는 KAFKA)
```

**LOG_MODE 설정 시:**
- `LOG_MODE=KAFKA`: `kafka()`를 호출하지 않아도 자동으로 Kafka 전송
- `LOG_MODE=FLUENTD`: `fluentd()`를 호출하지 않아도 자동으로 Fluentd 전송
- 설정하지 않으면: `fluentd()` 또는 `kafka()`를 명시적으로 호출해야 함

### 선택 (Kafka 설정, 기본값 있음)

```bash
KAFKA_SERVERS=kafka1:9092,kafka2:9092,kafka3:9092
KAFKA_TOPIC=bct-terminal-log
```

**기본값:**
- `KAFKA_SERVERS`: `133.186.213.152:39092,133.186.213.152:39093,133.186.213.152:39094`
- `KAFKA_TOPIC`: `unknown-terminal-log`

**Kafka 재시도:**
- Kafka 전송 실패 시 자동으로 재시도 큐에 추가
- 5초 간격으로 최대 3회 재시도 (현재 하드코딩되어 있습니다. 필요시 추후 환경변수 설정 방식으로 변경할 예정입니다)
- 재시도 실패 시 포기

## JSON 출력 예시

### logData가 Map인 경우

```json
{
  "level": "INFO",
  "logData": {
    "pinNo": "12345",
    "userName": "홍길동"
  },
  "serviceType": "TSS",
  "eventType": "GATEIN",
  "method": "myMethod",
  "line": 123,
  "traceId": "12345",
  "terminal": "HJNC",
  "message": "사용자 로그인",
  "controlViewYn": "N"
}
```

### logData가 String인 경우

```json
{
  "level": "INFO",
  "logData": "처리된 레코드 수: 100",
  "serviceType": "TSS",
  "eventType": "GATEIN",
  "method": "myMethod",
  "line": 123,
  "traceId": "",
  "terminal": "HJNC",
  "message": "작업 완료",
  "controlViewYn": "N"
}
```

### logData가 null인 경우

```json
{
  "level": "INFO",
  "logData": null,
  "serviceType": "TSS",
  "eventType": "GATEIN",
  "method": "myMethod",
  "line": 123,
  "traceId": "",
  "terminal": "HJNC",
  "message": "로그 메시지",
  "controlViewYn": "N"
}
```

### 기본 메서드에서 메시지 분리된 경우

**입력:** `logger.info("CopinoService [NIFI REV] : CopinoService(a=1,b=2)")`

```json
{
  "level": "INFO",
  "logData": " CopinoService(a=1,b=2)",
  "serviceType": "TSS",
  "eventType": null,
  "method": "myMethod",
  "line": 123,
  "traceId": "",
  "terminal": "HJNC",
  "message": "CopinoService [NIFI REV]",
  "controlViewYn": "N"
}
```

**입력:** `logger.info("test message")` (":" 없음)

```json
{
  "level": "INFO",
  "logData": "test message",
  "serviceType": "TSS",
  "eventType": null,
  "method": "myMethod",
  "line": 123,
  "traceId": "",
  "terminal": "HJNC",
  "message": "",
  "controlViewYn": "N"
}
```

### 메서드 체이닝에서 controlView() 호출한 경우

**입력:** `logger.message("프로세스 시작").logData(logData).info().controlView().kafka()`

```json
{
  "level": "INFO",
  "logData": {
    "pinNo": "12345"
  },
  "serviceType": "TSS",
  "eventType": null,
  "method": "myMethod",
  "line": 123,
  "traceId": "12345",
  "terminal": "HJNC",
  "message": "프로세스 시작",
  "controlViewYn": "Y"
}
```

## 필드 설명

- **level**: 로그 레벨 (ERROR, WARN, INFO, DEBUG)
- **logData**: 로그 데이터 (Map이면 중첩 JSON 객체, String이면 문자열, null 가능)
- **serviceType**: 서비스 타입 (메서드 체이닝 > `@ServiceType` 어노테이션 > 환경변수 SERVICE_TYPE > null)
- **eventType**: 이벤트 타입 (메서드 체이닝 > `@EventType` 어노테이션 > null, 언더바 제거 후 대문자 변환)
- **method**: 메서드명 (StackTrace에서 자동 추출)
- **line**: 라인 번호 (StackTrace에서 자동 추출)
- **traceId**: 추출된 traceId (logData가 Map이고 pinNo/docKey/workKey가 있을 때만 값이 있음, 없으면 빈 문자열)
- **terminal**: 터미널/IDC 식별자 (환경변수 SERVICE_TERMINAL > 메서드 체이닝 > null)
- **message**: 로그 메시지 (메서드 체이닝으로 설정)
- **controlViewYn**: 관제시스템 노출 여부 (Y 또는 N)
  - 기본 메서드(`info()`, `warn()`, `error()`, `debug()`): "N"
  - 메서드 체이닝에서 `controlView()` 호출 시: "Y"
  - 메서드 체이닝에서 `controlView()` 미호출 시: "N"

## traceId 추출 우선순위

logData가 Map인 경우, 다음 우선순위로 traceId를 추출합니다:

1. **최우선**: `pinNo`, `pinno`, `PINNO`
2. **차순위**: `dockey`, `docKey`, `DOCKEY`
3. **차차순위**: `workKey`, `workkey`, `WORKKEY`
4. **없으면**: 빈 문자열 `""`

## 마이그레이션 가이드

### 레거시 프로젝트 (Java 6 + Log4j)

**기존:**
```java
Logger logger = LoggerFactory.getLogger(getClass());
logger.info("로그메시지: " + 변수값);
```

**마이그레이션 방법 1: 직접 교체 (JSON 형태로 출력)**

```java
// Logger를 CustomLogger로 교체
CustomLogger logger = new CustomLogger(getClass());
logger.info("로그메시지: " + 변수값);  // JSON 형태로 출력됨
```

**마이그레이션 방법 2: 메서드 체이닝 사용 (Fluentd/Kafka 전송)**

```java
CustomLogger logger = new CustomLogger(getClass());

// 기본 메서드 (JSON 형태로 출력)
logger.info("일반 로그");

// 중요 로그만 Fluentd 또는 Kafka 전송
Map<String, Object> logData = new HashMap<String, Object>();
logData.put("key", "value");
logger.message("로그메시지")
    .logData(logData)
    .serviceType("TSS")
    .eventType("GATE_IN")
    .info()
    .fluentd();  // 또는 .kafka()

// 어노테이션 사용 (자동 설정)
@EventType("GATE_IN")
@ServiceType("TSS")
@LogMode("KAFKA")
public void myMethod() {
    CustomLogger logger = new CustomLogger(getClass());
    logger.message("로그메시지")
        .logData(logData)
        .info();  // eventType, serviceType이 자동 설정되고, kafka() 호출 없이도 Kafka 전송됨
}

// LOG_MODE 환경변수 사용
// export LOG_MODE=KAFKA
logger.message("로그메시지")
    .logData(logData)
    .info();  // 자동으로 Kafka 전송됨
```

### Spring Boot 프로젝트 (Java 11 + Logback)

동일한 방식으로 사용 가능합니다. 기존 logback.xml 설정은 그대로 유지됩니다.

## 동작 원리

### 기본 메서드

```java
logger.info("로그메시지");
logger.error("로그메시지");
logger.warn("로그메시지");
logger.debug("로그메시지");
```

- SLF4J Logger를 사용하여 기존 log4j.xml/logback.xml 설정대로 동작
- JSON 형태로 변환하여 출력
- `LOG_MODE` 환경변수에 따라 자동으로 Fluentd/Kafka 전송
- 메시지에 ":"가 있으면 자동으로 분리 (앞부분 → message, 뒷부분 → logData)
- 기존 로그 시스템에 영향 없음

### 메서드 체이닝 + .fluentd() 메서드 (Fluentd 전송)

```java
logger.message("로그메시지")
    .logData(mapData)
    .serviceType("TSS")
    .eventType("GATE_IN")
    .info()
    .fluentd();
```

1. **JSON 변환** - 로그 데이터를 JSON 형식으로 변환
2. **|fluentd| 프리픽스 추가** - Fluentd에서 구분할 수 있도록 프리픽스 추가
3. **기존 로그 시스템에 JSON 출력** - stdout + 파일에 JSON 저장
4. **Fluentd 수집** - Fluentd가 stdout을 수집하여 중앙 로그 시스템으로 전송
5. **설정 변경 불필요** - 기존 로그 설정 그대로 사용

### 메서드 체이닝 + .kafka() 메서드 (Kafka 전송)

```java
logger.message("로그메시지")
    .logData(mapData)
    .serviceType("TSS")
    .eventType("GATE_IN")
    .info()
    .kafka();
```

1. **JSON 변환** - 로그 데이터를 JSON 형식으로 변환
2. **기존 로그 시스템에 JSON 출력** - stdout + 파일에 JSON 저장 (프리픽스 없음)
3. **Kafka 전송** - 비동기로 JSON 형태로 Kafka에 전송
4. **에러 처리** - Kafka 전송 실패해도 애플리케이션은 계속 동작

## 프로젝트 구조

```
unified-logging/
├── pom.xml
├── README.md
└── src/
    └── main/
        └── java/
            └── com/smartm2m/logging/
                ├── CustomLogger.java          # 메인 로거
                ├── EventType.java             # eventType 어노테이션
                ├── ServiceType.java           # serviceType 어노테이션
                ├── LogMode.java               # logMode 어노테이션
                ├── KafkaProducerManager.java  # Kafka Producer 관리 (싱글톤)
                ├── JsonFormatter.java         # JSON 변환
                └── TraceIdExtractor.java      # TraceId 추출
```

## 빌드 및 배포

```bash
# 컴파일
mvn clean compile

# 패키징 (JAR 생성)
mvn clean package

# 소스 JAR 포함 빌드
mvn clean package source:jar

# Nexus Repository에 배포
mvn clean deploy
```

## 트러블슈팅

### Kafka 연결 실패

- 환경변수 `KAFKA_SERVERS` 확인
- Kafka 전송 실패해도 애플리케이션은 계속 동작
- 기존 로그는 정상적으로 남음

### 서비스 정보 null

- 환경변수 `SERVICE_TERMINAL`, `SERVICE_TYPE` 설정 확인
- 또는 메서드 체이닝으로 직접 설정

### 성능 문제

- Kafka 전송은 비동기로 동작 (메인 스레드 블로킹 없음)
- 기존 로그 시스템에 영향 없음
- 모든 로그는 JSON 형태로 통일되어 Fluentd 수집이 일관됨

### 컴파일 에러 (Kafka client)

- Java 6 환경에서는 Kafka client 0.9.0.0 이상 필요
- Java 7+ 환경에서는 최신 버전 사용 가능

---

**Version**: 1.1.2  
**Last Updated**: 2025-01-15
