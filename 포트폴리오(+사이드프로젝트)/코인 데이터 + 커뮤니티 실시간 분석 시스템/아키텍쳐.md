
# 코인 데이터 & 커뮤니티 실시간 분석 시스템

## 1. 프로젝트 개요

### 목적
- 업비트 코인 시세와 커뮤니티(디시인사이드) 데이터 연계 분석
- 실시간/배치 처리가 결합된 데이터 파이프라인 구축
- 데이터 엔지니어링 핵심 기술 스택 습득
- AWS Free Tier를 활용한 비용 효율적 시스템 구축

### 수집 데이터
1. 코인 데이터 (업비트)
   - 실시간 시세 데이터
   - WebSocket을 통한 실시간 수집
   - 주요 코인 10-20개 대상

2. 커뮤니티 데이터 (디시인사이드)
   - 게시글, 댓글 데이터
   - 5분 주기 크롤링
   - 코인 관련 게시판 대상

### 주요 기능
1. 실시간 데이터 처리
   - 코인 시세 스트리밍 처리
   - 1분봉 데이터 생성
   - 실시간 데이터 저장

2. 배치 데이터 처리
   - 커뮤니티 데이터 정기 수집
   - 데이터 정제 및 분석
   - 주기적 데이터 백업

3. 데이터 수명주기 관리
   - RDS: 최근 2주 데이터
   - S3: 2주~2개월 데이터
   - 로컬: 2개월 이상 데이터

## 2. 기술 스택 & 아키텍처

### 사용 기술
1. AWS 서비스
   - Lambda: 실시간 데이터 수집
   - RDS (PostgreSQL): 최근 데이터 저장
   - S3: 중기 데이터 저장

2. 데이터 처리
   - Kafka: 실시간 메시지 큐
   - Spark: 대용량 데이터 처리
   - Airflow: 배치 작업 관리

3. 개발 언어
   - Java: Lambda, Kafka, Spark
   - Python: Airflow

### 상세 아키텍처

1. 데이터 수집 계층
```
[실시간 수집]
업비트 WebSocket → Lambda(Java) → Kafka
- WebSocket 연결 관리
- 실시간 가격 데이터 처리

[배치 수집]
디시인사이드 → Lambda(Java) → Kafka
- 5분 주기 크롤링
- 커뮤니티 데이터 수집
```

2. 데이터 처리 계층
```
Kafka → Spark(Java)
- 스트리밍 데이터 처리
- 데이터 정제/분석
- RDS 저장
```

3. 데이터 저장 계층
```
[데이터 저장]
Spark → RDS (최근 2주)
    └→ S3 (2주~2개월)
    └→ Local Storage (2개월 이상)

[데이터 이관]
Airflow
- RDS → S3 (Daily)
- S3 → Local (Monthly)
```

## 3. 개발 계획

### Phase 1: 기본 인프라 구축 (2주)
1. AWS 환경 설정
   - Lambda 함수 생성
   - RDS, S3 설정
   - 보안 그룹 설정

2. 기본 환경 구축
   - Kafka 클러스터 설정
   - Spark 환경 구성
   - Airflow 설치

### Phase 2: 데이터 수집 구현 (2주)
1. 실시간 데이터 수집
   - 업비트 WebSocket 연동
   - Lambda 코드 구현

2. 배치 데이터 수집
   - 디시인사이드 크롤러 구현
   - Lambda 스케줄링 설정

### Phase 3: 데이터 처리/저장 (2주)
1. Kafka 스트리밍
   - 토픽 설계
   - Producer/Consumer 구현

2. Spark 처리
   - 실시간 처리 로직
   - 데이터 정제/분석

3. 데이터 저장
   - RDS 스키마 설계
   - 저장 프로세스 구현

### Phase 4: 데이터 관리 (1주)
1. Airflow 파이프라인
   - 데이터 이관 작업
   - 백업 프로세스
   - 모니터링 설정



# 프로젝트 수행 순서

1⃣ 첫 단계: 데이터 수집 부분 
- 업비트 WebSocket 연결
- 실시간 데이터 수집 테스트
- 간단한 콘솔 출력으로 확인
```
목표: 코인 데이터가 지속적으로 잘 들어오는지 확인
```

2⃣ 두번째: 기본 저장소 설정
- RDS 설정
- 간단한 테이블 구성
- 데이터 저장 테스트
```
목표: 수집한 데이터를 저장할 곳 마련
```

3⃣ 세번째: 디시인사이드 크롤링
- 크롤러 구현
- 데이터 정제
- RDS 저장 테스트
```
목표: 커뮤니티 데이터 수집 파이프라인 구축
```

4⃣ 그 다음: 실시간 처리 환경
- Kafka 설정
- Spark 환경 구축
- 기본 처리 로직 구현
```
목표: 실시간 데이터 처리 파이프라인 구축
```